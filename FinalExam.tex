\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amssymb,amsfonts}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\e}{\varepsilon}
\newcommand{\lra}{\Longrightarrow}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
  \item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2:}]}{\end{trivlist}}
\newenvironment{subproblem}[2][Part]{\begin{trivlist}
  \item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries (#2)}]}{\end{trivlist}}
\newenvironment{solution}[1][Solution]{\begin{trivlist}
  \item[\hskip \labelsep {\bfseries #1} \hskip \labelsep]}{\end{trivlist}}

\theoremstyle{remark}
\newtheorem*{theorem*}{Theorem}

\begin{document}

\title{Math 4990 - Final Exam}
\author{Kevin Kim t00201473}
\maketitle

\begin{problem}{1}
  Consider the integral function
  \[
    J[y] = \int_{1}^{2}\frac{y'(x)^2}{x}dx
  \]
  The integrand here is strongly convex (on an appropriately defined set).
  Find the unique \(y \in D\) that minimizes \(J[y]\) over D, for the following cases.
  In each case, is the minimizer (if it exists) unique?
\end{problem}

\begin{subproblem}{a}
  \(D=\{y \in C^1[1,2]:y(1) = 0, y(2) = 3\}\)
\end{subproblem}

\begin{solution}
  $ $\\
  If J is a minimum at \(y \in D \) then \(y\) must satify the Euler-Lagrange equation
  \\\(F_y(x,y,y') - \frac{d}{dx}F_{y'}(x,y,y') = 0\) where \(F(x,y,y') = \frac{y'(x)^2}{x}\).
  \begin{align*}
    \frac{\partial F}{\partial y} - \frac{d}{dx}\frac{\partial F}{\partial y'} = 0
    &\lra \frac{\partial F}{\partial y} = \frac{d}{dx}\frac{\partial F}{\partial y'} \\
    &\lra 0 = \frac{d}{dx}\frac{2y'}{x} \lra \int 0 dx = \int \frac{d}{dx}\frac{2y'}{x} dx \\
    &\lra \frac{2y'}{x} = C \quad (C \in \R) \\
    &\lra y' = Bx \quad (B \in \R) \\
    &\lra \int y' dx = \int Bx dx \\
    &\lra y = c_1x^2 + c_2 \quad (c1,c2 \in \R)
  \end{align*}
  \(y\) must also satisfy the boundary conditions:
  \[
    \left\{
      \begin{array}{ll}
        y(1) = c_1 + c_2 = 0\\
        y(2) = 4c_1 + c_2 = 3\\
      \end{array}
    \right.
    \lra c_1 = \frac{3}{5}, c_2 = -\frac{3}{5} \lra \boxed{y = \frac{3}{5}x^2 - \frac{3}{5}}
  \]
  We have shown with Euler-Lagrange equation the necessary conditions for \(y\) to minimize \(J[y]\)
  and because we are given the integrand is strongly convex on the defined set D, we have
  the sufficient conditions to guarantee \(y\) is the unique minimizer.
\end{solution}
\clearpage
\begin{subproblem}{b}
  \(D = \{y \in C^1[1,2]:y(2) = 3\}\)
\end{subproblem}

\begin{solution}
  $ $\\
  From the notes given in class, we have the following:
  \begin{theorem*}{}
    If \(F(x,y,y')\) is strongly convex in \((y,y')\) then a solution \(y(x)\) of the
    Differential Euler-Lagrange equation uniquely minimizes
    \[
      J[y] = \int_{a}^{b}F(x,y,y')dx
    \]
    over \(\{y \in C'[a,b] : y(b) = B\} \) if \(F_{y'}(a,y(a),y'(a)) = 0\).
  \end{theorem*}
  $ $\\
  We find the solution to the Euler-Lagrange equation \(F_y(x,y,y') -
  \frac{d}{dx}F_{y'}(x,y,y') = 0\) where \(F(x,y,y') = \frac{y'(x)^2}{x}\).
  \begin{align*}
    \frac{\partial F}{\partial y} - \frac{d}{dx}\frac{\partial F}{\partial y'} = 0
    &\lra \frac{\partial F}{\partial y} = \frac{d}{dx}\frac{\partial F}{\partial y'} \\
    &\lra 0 = \frac{d}{dx}\frac{2y'}{x} \lra \int 0 dx = \int \frac{d}{dx}\frac{2y'}{x} dx \\
    &\lra \frac{2y'}{x} = C \quad (C \in \R) \\
    &\lra y' = Bx \quad (B \in \R) \\
    &\lra \int y' dx = \int Bx dx \\
    &\lra y = c_1x^2 + c_2 \quad (c1,c2 \in \R)
  \end{align*}
  \(y\) must also satisfy the boundary condition:
  \begin{align*}
    y(2) = 4c_1 + c_2 = 3
    &\lra c_2 = 3-4c_1 \\
    &\lra y = c_1x^2 + (3-4c_1) \\
    &\lra y = c_1(x^2-4)+3
  \end{align*}
  thus
  \begin{align*}
    F_{y'}(1,y(1),y'(1)) = 0 &\lra \frac{2y'(1)}{1} = 0 \\
                             &\lra y'(1) = 0 \\
                             &\lra 2c_1(1) \\
                             &\lra c_1 = 0
  \end{align*}
  which implies
  \begin{align*}
    \boxed{y(x) = 3}
  \end{align*}
  Given the integrand is strongly convex on the defined set \(D\) along with the theorem
  mentioned above, \(y\) uniquely minimizes \(J[y]\) over \(D\).
\end{solution}

\clearpage
\begin{subproblem}{c}
  \(D = C^1[1,2]\)
\end{subproblem}
\begin{solution}
  $ $\\
  If J is a minimum at \(y \in D \) then \(y\) must satify the Euler-Lagrange equation
  \\\(F_y(x,y,y') - \frac{d}{dx}F_{y'}(x,y,y') = 0\) where \(F(x,y,y') = \frac{y'(x)^2}{x}\).
  \begin{align*}
    \frac{\partial F}{\partial y} - \frac{d}{dx}\frac{\partial F}{\partial y'} = 0
    &\lra \frac{\partial F}{\partial y} = \frac{d}{dx}\frac{\partial F}{\partial y'} \\
    &\lra 0 = \frac{d}{dx}\frac{2y'}{x} \lra \int 0 dx = \int \frac{d}{dx}\frac{2y'}{x} dx \\
    &\lra \frac{2y'}{x} = C \quad (C \in \R) \\
    &\lra y' = Bx \quad (B \in \R) \\
    &\lra \int y' dx = \int Bx dx \\
    &\lra y = c_1x^2 + c_2 \quad (c1,c2 \in \R)
  \end{align*}
  From strong convexity and observation we see if \(y = c_2\), we achieve a minimum for \\
  \(J[y] = \int_{1}^{2}\frac{y'(x)^2}{x}dx\) since \(y'(x)^2 \ge 0\) and
  \(x \in [1,2]\) which implies \(J[y] \ge 0\). However \(y\) is not a unique minimizer
  since \(c2 \in \R\).

\end{solution}
\clearpage
\begin{problem}{2}
  Let \(\delta(x)\) be the Dirac delta function. Justify the identity
  \[
    \delta(1-x^2) = \frac{\delta(x-1)+\delta(x+1)}{2}
  \]
  . \\
  Hint: consider the integral \(\int_{-\infty}^{\infty}\delta(1-x^2)dx\)
\end{problem}
\begin{solution}
  For any continuous function f(x), we have
  \[
    \int_{\infty}^{\infty}f(x)\delta(1-x^2)dx =
    \underbrace{\int_{0}^{\infty}f(x)\delta(1-x^2)dx}_\text{(1)} +
    \underbrace{\int_{-\infty}^{0}f(x)\delta(1-x^2)dx}_\text{(2)}
  \]
  If we let \(\sqrt{u} = x\) and \(\frac{1}{2\sqrt{u}}du = dx\)
  \[
    (1) = \int_{0}^{\infty}\frac{f(\sqrt{u})\delta(1-u)}{2\sqrt{u}}du = \frac{f(1)}{2}
  \]
  If we let \(-\sqrt{u} = x\) and \(-\frac{1}{2\sqrt{u}}du = dx\)
  \[
    (2) = -\int_{-\infty}^{0}\frac{f(-\sqrt{u})\delta(1-u)}{2\sqrt{u}}du
    = \int_{0}^{\infty}\frac{f(-\sqrt{u})\delta(1-u)}{2\sqrt{u}}du
    = \frac{f(-1)}{2}
  \]
  thus
  \begin{align*}
    \int_{\infty}^{\infty}f(x)\delta(1-x^2)dx
    &= \frac{1}{2}[f(1) + f(-1)] \\
    &= \frac{1}{2}\bigg[\int_{-\infty}^{\infty}f(x)\delta(x-1)dx +
      \int_{-\infty}^{\infty}f(x)\delta(x+1)dx\bigg] \\
    &= \frac{1}{2}\int_{-\infty}^{\infty}f(x)(\delta(x-1) + \delta(x+1)) dx \\
    &= \int_{-\infty}^{\infty}f(x)\frac{\delta(x-1) + \delta(x+1)}{2} dx
  \end{align*}
  Since \(\delta\) functions are defined only by how they behave in integrals we conclude
  \[
    \boxed{\delta(1-x^2) = \frac{\delta(x-1)+\delta(x+1)}{2}}
  \]
\end{solution}
\clearpage
\begin{problem}{3}
  Let \(f : [0,1] \rightarrow \R\) be a given continuous function
  and consider the following differential equation for the function \(y\):
  \begin{align}
    \left\{
    \begin{array}{ll}
      y'''(x) = f(x) \qquad\qquad\qquad 0 \le x \le 1 \\
      y(0) = y'(0) = y''(0) = 0
    \end{array}
    \right.
  \end{align}
  Intuitively, finding \(y(x)\) requires three integrations. This problem illustrates that
  by employing a Green's function we can actually find \(y\) via a $single$ integral.
\end{problem}
\begin{subproblem}{a}
  Show that if the Green's function \(u = G(x;s)\) satisfies
  \[
    \left\{
      \begin{array}{ll}
        u'''(x) = \delta(x-s) \qquad\qquad\qquad 0 \le x \le 1 \\
        u(0) = u'(0) = u''(0) = 0
      \end{array}
    \right.
  \]
  then the function
  \begin{align}
    y(x) = \int_{0}^{1}f(s)G(x;s)ds
  \end{align}
  satisfies (1).
\end{subproblem}
\begin{solution}
  $ $\\
  From the notes given in class, we have the following:
  \begin{theorem*}{}
    $ $\\
    The function \(y(x) = \int_{0}^{1}f(s)G(x;s)ds\)
    satisfies the differential equation
    \[
      \left\{
        \begin{array}{ll}
          L[y] = y'''(x) = f(x) \qquad\qquad\qquad 0 \le x \le 1 \\
          y(0) = y'(0) = y''(0) = 0
        \end{array}
      \right.
    \]
  \end{theorem*}
  $ $\\
  Let us first observe
  \begin{align*}
    L[y] = y''' &= \frac{d^3}{dx^3}\big(\int_{0}^{1}f(s)G(x;s)ds\big) \\
                &= \int_{0}^{1}\frac{d^3}{dx^3}\big(f(s)G(x;s)\big)ds \\
                &= \int_{0}^{1}f(s)\frac{d^3}{dx^3}\big(G(x;s)\big)ds \\
                &= \int_{0}^{1}f(s)G'''(x;s)ds \\
                &= \int_{0}^{1}f(s)L[G]ds \\
  \end{align*}
  and
  \begin{align*}
    L[G] = G'''(x;s) = \delta(x-s), \qquad 0 \le x \le 1
  \end{align*}
  thus
  \begin{align*}
    L[y] &= L\bigg[\int_{0}^{1}f(s)G(x;s)ds \bigg] \\
         &= \int_{0}^{1}L\bigg[f(s)G(x;s)\bigg] ds \\
         &= \int_{0}^{1}f(s)L[G]ds \\
         &= \int_{0}^{1}f(s)\delta(x-s)ds \\
         &= f(x)
  \end{align*}
  along with the boundary conditions given \(G(0;s) = 0\)
  \begin{align*}
    y(0) &= \int_{0}^{1}f(s)\underbrace{G(0;s)}_{\text 0}ds = 0 \\
    y'(0) &= \int_{0}^{1}f'(s)\underbrace{G(0;s)}_{\text 0}ds = 0 \\
    y''(0) &= \int_{0}^{1}f''(s)\underbrace{G(0;s)}_{\text 0}ds = 0 \\
  \end{align*}
  We have showed (2) satisfies (1).
\end{solution}
\clearpage
\begin{subproblem}{b}
  Find the Green's function \(G(x:s)\) for this problem
  \[
    \left\{
      \begin{array}{ll}
        y'''(x) = f(x) \qquad\qquad\qquad 0 \le x \le 1 \\
        y(0) = y'(0) = y''(0) = 0
      \end{array}
    \right.
  \]
\end{subproblem}

\begin{solution}
  To find Green's function we solve for
  \[
    \left\{
      \begin{array}{ll}
        u'''(x) = \delta(x-s) \qquad\qquad\qquad 0 \le x \le 1 \\
        u(0) = u'(0) = u''(0) = 0
      \end{array}
    \right.
  \]
  where \(u(x) = G(x;s)\).
  $ $\\
  $ $\\
  For \(x \ne s\) the differential equation is
  \[
    u'''(x) = 0 \qquad\qquad\qquad 0 \le x \le 1
  \]
  whose general solution is found to be
  \[
    u(x) = Ax^2 + Bx + C \qquad (A,B,C \in \R)
  \]
  thus
  \[
    u(x) =
    \left\{
      \begin{array}{ll}
        Ax^2 + Bx + C \qquad x < s \\
        Ex^2 + Dx + F \qquad x > s
      \end{array}
    \right.
  \]
  The boundary conditions require
  \begin{alignat*}{3}
    u(0) = 0 \quad &\lra Ax^2 +Bx + C = 0 \quad &\lra C = 0\\
    u'(0) = 0 \quad &\lra 2Ax + B = 0 \quad &\lra B = 0\\
    u''(0) = 0 \quad &\lra 2A = 0 \quad &\lra A = 0.
  \end{alignat*}
  The Green function now has the form
  \begin{align*}
    u(x) =
    \left\{
    \begin{array}{ll}
      0 &x < s \\
      Ex^2 + Dx + F &x > s.
    \end{array}
                      \right.
  \end{align*}
  We also need to satisfy the second derivative discontinuity condition since \\
  \(u''' = \delta(x-s) \Rightarrow u'' = H(x-s)\)
  \begin{align*}
    u''(s^+)-u''(s^-) = 1 &\lra 2E - 0 = 1 \\
                          &\lra 2E = 1 \\
                          &\lra E = \frac{1}{2}
  \end{align*}
  The Green function now has the form
  \begin{align*}
    u(x) =
    \left\{
    \begin{array}{ll}
      0 &x < s \\
      \frac{1}{2}x^2 + Dx + F &x > s.
    \end{array}
                                \right.
  \end{align*}
  Together with the need to satisfy the first derivative continuity condition since \\
  \(u' = \int H(x-s) dx\) is continuous.
  \begin{align*}
    u'(s^+) = u''(s^-) &\lra s + D = 0 \\
                       &\lra D = -s
  \end{align*}
  The Green function now has the form
  \begin{align*}
    u(x) =
    \left\{
    \begin{array}{ll}
      0 &x < s \\
      \frac{1}{2}x^2 - sx + F &x > s.
    \end{array}
                                \right.
  \end{align*}
  Along with the continuity condition of \(u\) since \(u'\) is continuous which implies
  \(u\) is continuous.
  \begin{align*}
    u(s^+) = u(s^-) &\lra \frac{1}{2}s^2 - s^2 + F = 0 \\
                    &\lra -\frac{1}{2}s^2 + F = 0 \\
                    &\lra F = \frac{1}{2}s^2.
  \end{align*}
  Thus the Green function is as follows
  \begin{align*}
    \boxed{
    u(x) =
    \left\{
    \begin{array}{ll}
      0 &x < s \\
      \frac{1}{2}(x^2+s^2) - sx &x > s
    \end{array}
                                  \right.}
  \end{align*}
\end{solution}
\clearpage
\begin{subproblem}{c}
  For the case \(f(x) = x\), solve (1) directly by integrating thrice. Verify that (2)
  gives the same result.
\end{subproblem}

\begin{solution}
  We are given
  \begin{align*}
    \left\{
    \begin{array}{ll}
      y'''(x) = x \qquad\qquad\qquad 0 \le x \le 1 \\
      y(0) = y'(0) = y''(0) = 0
    \end{array}
    \right.
  \end{align*}
  By integrating \(f(x) = x\) thrice,
  \begin{align*}
    \iiint x dx &\lra \iint \frac{x^2}{2} + c_1 dx \\
                &\lra \int \frac{x^3}{6} + c_1x + c_2 dx \\
                &\lra \frac{x^4}{24} + \frac{c_1x^2}{2} + c_2x + c_3
                  \qquad (c_1,c_2,c_3 \in R)
  \end{align*}
  and in order to satisfy the boundary conditions, \(c_1 = c_2 = c_3 = 0\) since
  \begin{align*}
    \left\{
    \begin{array}{ll}
      y(0) &= \frac{0^4}{24} = 0 \\
      y'(0) &= \frac{0^3}{6} = 0 \\
      y''(0) &= \frac{0^2}{3} = 0.
    \end{array}
               \right.
  \end{align*}
  Thus
  \[
    y = \frac{x^4}{24}.
  \]
  Now to verify green's function with \(y(x) = \int_{0}^{1}f(s)G(x;s)ds\), we have
  \[
    G(x;s) =
    \left\{
      \begin{array}{ll}
        0 &x < s \\
        \frac{1}{2}(x^2+s^2) - sx &x > s
      \end{array}
    \right.
  \]
  therefore
  \begin{align*}
    y(x) &= \int_{0}^{x}s\big( \frac{1}{2}(x^2+s^2) - sx \big)ds + \int_{x}^{1}0ds \\
         &= \int_{0}^{x}\frac{sx^2}{2} + \frac{s^3}{2} - s^2x ds \\
         &= \bigg[\frac{s^2x^2}{4} + \frac{s^4}{8} - \frac{s^3x}{3} \bigg]_0^x \\
         &= \frac{x^4}{4} + \frac{x^4}{8} - \frac{x^4}{3} \\
         &= \frac{x^4}{24}.
  \end{align*}
  We have showed both methods give the same results.
\end{solution}
\clearpage
\begin{problem}{4}
  The Fourier transform is often useful for finding Green's functions. Consider the following
  partial differential equation for the function \(u(x,t)\):
  \begin{align*}
    \left\{
    \begin{array}{ll}
      u_{tt}-u_{xx} + u = 0 & -\infty < x < \infty \\
      u(x,0) = \delta(x-s) & s \in \R \\
      u_t(x,0) = 0.
    \end{array}
    \right.
  \end{align*}
\end{problem}
\begin{subproblem}{a}
  Apply a Fourier transform with respect to \(x\) to obtain a 2nd-order ordinary differential
  equation for the function \(\hat{u}(\omega,t)\).
\end{subproblem}
\begin{solution}
  $ $\\
  By applying the Fourier transform with respect to x such that \(\F\{u(x,t)\} =
  \hat{u}(\omega,t)\) we get
  \begin{align*}
    \left\{
    \begin{array}{ll}
      u_{tt}-u_{xx} + u &= 0 \\
      u(x,0) &= \delta(x-s) \\
      u_t(x,0) &= 0
    \end{array}
                 \right.
                 \quad\xrightarrow{\F}\quad
                 \left\{
                 \begin{array}{ll}
                   \hat{u}_{tt} - (i\omega)^2\hat{u} + \hat{u} &= 0 \qquad\qquad^{(1)}\\
                   \hat{u}(\omega,0) &= \frac{1}{2\pi}e^{-i\omega s} \qquad^{(2)}\\
                   \hat{u}_t(\omega,0) &= 0.
                 \end{array}
                                         \right.
  \end{align*}
  which is a 2nd order ordinary differential equation. Second order due to \(\hat{u}_{tt}\)
  and ordinary due to derivatives only involving \(t\). \\
  $ $ \\
  \begin{footnotesize}
    \(^{(1)}\)By derivative property of Fourier Transforms. \\
    \(^{(2)}\)By shift property and Fourier Transform of the Dirac Delta function.
  \end{footnotesize}
\end{solution}

\begin{subproblem}{b}
  Find \(\hat{u}(\omega,t)\) by solving the differential equation from part (a).
\end{subproblem}
\begin{solution}
  From part a we have the following:
  \[
    \left\{
      \begin{array}{ll}
        \hat{u}_{tt} - (i\omega)^2\hat{u} + \hat{u} &= 0 \\
        \hat{u}(\omega,0) &= \frac{1}{2\pi}e^{-i\omega s} \\
        \hat{u}_t(\omega,0) &= 0.
      \end{array}
    \right.
  \]
  For \(\hat{u}_{tt} - (i\omega)^2\hat{u} + \hat{u} = 0\),
  \begin{align*}
    \hat{u}_{tt} = (i\omega)^2\hat{u} - \hat{u} &\lra \hat{u}_{tt} = (-\omega^2 - 1)\hat{u} \\
                                                &\lra \hat{u} = c_1e^{\sqrt{-\omega^2-1}t}+
                                                  c_2e^{-\sqrt{-\omega^2-1}t}
                                                  \quad (c_1,c_2 \in \R)\qquad^{(1)}
  \end{align*}
  \begin{footnotesize}
    \(^{(1)}\)By solving for second-order linearly ordinary differential
    equation on Wolfram Alpha \\
  \end{footnotesize}
  $ $\\
  together with \(\hat{u}_t(\omega,0) = 0\)
  \begin{align*}
    \hat{u}_t(\omega,0) = 0 &\lra \frac{d}{dt}(c_1e^{\sqrt{-\omega^2-1}t}+ c_2e^{-\sqrt{-\omega^2-1}t}) = 0\\
                            &\lra c_1\sqrt{-\omega^2-1}e^{\sqrt{-\omega^2-1}(0)} -
                              c_2\sqrt{-\omega^2-1}e^{-\sqrt{-\omega^2-1}(0)} = 0 \\
                            &\lra c_1\sqrt{-\omega^2-1} = c_2\sqrt{-\omega^2-1} \\
                            &\lra c_1 = c_2
  \end{align*}
  along with \(\hat{u}(\omega,0) = \frac{1}{2\pi}e^{-i\omega s}\)
  \begin{align*}
    &\lra \frac{1}{2\pi}e^{-i\omega s} \underbrace{\bigg(c_1e^{\sqrt{-\omega^2-1}(0)}+
      c_1e^{-\sqrt{-\omega^2-1}(0)} \bigg)}_{= 1} \\
    &\lra c_1 = \frac{1}{2}
  \end{align*}
  therefore
  \begin{align*}
    \hat{u}(\omega,t) &= \frac{1}{4\pi}e^{-i\omega s}\bigg(e^{\sqrt{-\omega^2-1}t}+
                        e^{-\sqrt{-\omega^2-1}t} \bigg) \\
                      &= \boxed{\frac{1}{2\pi}e^{-i\omega s}cosh\big(t\sqrt{-\omega^2-1} \big)}
                        \qquad \text{since } e^x + e^-x = 2\text{cosh}(x)\\
  \end{align*}
\end{solution}
\begin{subproblem}{c}
  Invert the Fourier transform to obtain \(u(x,t)\)
\end{subproblem}
\begin{solution}
  \begin{align*}
    u(x,t) &= \int_{-\infty}^{\infty}\hat{u}(\omega,t)e^{i\omega t}d\omega \\
           &= \int_{-\infty}^{\infty}\bigg[\frac{1}{2\pi}\text{cosh}\big(t\sqrt{-\omega^2-1} \big)
             \bigg]e^{i\omega t}d\omega \\
           &= \frac{1}{2\pi}\int_{-\infty}^{\infty}\text{cosh}\big(t\sqrt{-\omega^2-1} \big)
             e^{i\omega (t-s)}d\omega \\
  \end{align*}
\end{solution}
\clearpage
\begin{problem}{5}
  Consider the cubic equation
  \begin{align}
    x^3 - x + \e = 0
  \end{align}
  with \(|\e| \ll 1\). Assume a series solution of the form
  \[
    x(\e) = x_0+x_1\e+x_2\e^2 + ...
  \]
  Substitute this onto (3) and consider the limit \(\e \rightarrow 0\) to determine the
  coefficients \(x_0,x_1,x_2\) and thereby obtain a three-term asymptotic approximation for
  each of the three roots. Verify that your series gives reasonably accurate results for the
  cases \(\e = 0.1\) and \(\e = 0.01\).
\end{problem}
\begin{solution}
  By substituting \(x(\e) = x_0+x_1\e+x_2\e^2 + ... \) into \(x^3 - x + \e = 0\) we get
  \begin{align*}
    x(\e)^3 - x(\e) + \e \lra& (x_0+x_1\e+x_2\e^2 + ...)^3 - (x_0+x_1\e+x_2\e^2 + ...) + \e \\
                         \lra& ({x_0}^3+3{x_0}^2{x_1}\e+3{x_0}^2x_2e^2+3x_0{x_1}^2\e^2+ 6x_0x_1x_2e^3+\\
                             &3x_0{x_2}^2\e^4+{x_1}^3\e^3+3{x_1}^2{x_2}\e^4+3x_1{x_2}^2\e^5+{x_2}^3\e^6) \\
                             &- (x_0+x_1\e+x_2\e^2) +\e  = 0\\
  \end{align*}
  For \(\e^0\) terms:
  \[
    {x_0}^3-x_0 = 0 \lra
    \left\{
      \begin{array}{ll}
        x_0 = -1\\
        x_0 = 0\\
        x_0 = 1\\
      \end{array}
    \right.
  \]
  For \(\e^1\) terms:
  \[
    3{x_0}^2x_1 - x_1 + 1 = 0 \lra
        \left\{
      \begin{array}{ll}
        x_0 = \pm1 \lra 3x_1 - x_1 + 1 = 0 &\lra x_1 = -\frac{1}{2}\\
        x_0 = 0 \lra - x_1 + 1 = 0 &\lra x_1 = 1\\
      \end{array}
    \right.
  \]
  For \(\e^2\) terms:
  \[
    3{x_0}^2x_2+3x_0{x_1}^2-x_2 = 0\lra \\
    \left\{
      \begin{array}{ll}
        x_0 = 1, x_1 = -\tfrac{1}{2} \lra 3x_2 + \tfrac{3}{4} - x_2 = 0 &\lra x_2 = -\tfrac{3}{8} \\
        x_0 = -1, x_1 = -\tfrac{1}{2} \lra 3x_2 - \tfrac{3}{4} - x_2 = 0 &\lra x_2 = \tfrac{3}{8} \\
        x_0 = 0, x_1 = 1 &\lra x_2 = 0
      \end{array}
    \right.
  \]
  Thus the three-term asymptotic approximation for each of the three roots are:
  \[
    x = 
    \left\{
      \begin{array}{ll}
        1-\tfrac{1}{2}\e-\tfrac{3}{8}\e^2 \\
        -1-\tfrac{1}{2}\e+\tfrac{3}{8}\e^2 \\
        \e
      \end{array}
    \right.
  \]
  \textbf{Verification}
  $ $\\
  For \(\e\) = 0.1, \(x^3 - x + 0.1 = 0\), 
  \[
    x = 0.945649, \quad    
    x = -1.04668, \quad
    x = 0.101031, \quad
  \]
  \[
    \left\{
      \begin{array}{ll}
        1-\tfrac{1}{2}(0.1)-\tfrac{3}{8}(0.1)^2  &= 0.94625\\
        -1-\tfrac{1}{2}(0.1)+\tfrac{3}{8}(0.1)^2 &= -1.04625\\
        0.1 &= 0.1
      \end{array}
    \right.
  \]
  For \(\e\) = 0.01, \(x^3 - x + 0.01 = 0\), 
  \[
    x = 0.994962, \quad    
    x = -1.00496, \quad
    x = 0.010001, \quad
  \]
  \[
    \left\{
      \begin{array}{ll}
        1-\tfrac{1}{2}(0.1)-\tfrac{3}{8}(0.1)^2  &= 0.9949625\\
        -1-\tfrac{1}{2}(0.1)+\tfrac{3}{8}(0.1)^2 &= -1.004962\\
        0.01 &= 0.01
      \end{array}
    \right.
  \]
  $ $\\
  So we get reasonably accurate results for cases \(\e = 0.1\) and \(\e = 0.01\)

\end{solution}

\end{document}
